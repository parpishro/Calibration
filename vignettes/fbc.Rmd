---
title: "FBC: Full Bayesian Calibration"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 2
    fig_caption: true
vignette: >
  %\VignetteIndexEntry{FBC: Full Bayesian Calibration}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = ">"
)
options(width = 100)
```


## Introduction

#' This implementation is based on Kennedy-O'Hagan (KOH) calibration model. In
#' their seminal paper*, Kennedy and O'Hagan augmented simulator output with
#' field observation and fit a three-component model that accounts for simulator
#' input and bias correction using two independent Gaussian Processes (GP) and a
#' third term representing measurement error.
#' \deqn{z_i = \eta (x_i, \kappa) + \delta(x_i) + e_i}
#' In the original paper, the authors proposed a two-stage hierarchical Bayesian
#' model. In the first stage, point estimates of GP hyperparameters are computed
#' using maximum likelihood estimation (MLE) method. In the second stage, these
#' hyperparameters are fixed at their estimated value and run a MCMC algorithm
#' to sample calibration parameters.
#' In contrast, `calibrate` runs MCMC algorithm to sample the posterior
#' distribution of all parameters/hyperparameters. As a result, priors must be
#' specified carefully to reflect the prior expert belief about the distribution
#' and initial values must be chosen as close as possible to prior means.

`FBC` is a package that uses the data from both field experiment and computer simulation to calibrate the simulator model [^1]. A physical experiment relates a set of experimental variables as inputs to a response as output. A computer experiment also relates the same experimental variables as inputs to the response as output but also include calibration inputs. These inputs either represent unknown but fixed physical properties that are governed by the physical system in field experiments (and therefore need not be specified in field experiments) or represent various aspects of the simulation model such as tuning hyperparameter of the model. In both cases, the calibration parameters must be estimated for a simulator model to mimic the physical system adequately. The goal of the calibration is to estimate calibration parameters by fitting a statistical model to the observed data using both field and simulator data. Estimated parameters of the calibration model can be used for inference or calibrated prediction. 

<p>This is a <em>paragraph</em> with emphasized text.</p>
\

[^1]: *Field experiments, which are sometimes called physical experiments in other texts, will be represented by f subscript (for field) and computer simulations, which are sometimes called computer experiments in other texts, will be represented by s subscript (for simulation) throughout this vignette.*</em>.


The `FBC` package is a based on the well-known Kennedy and O'Hagan (KOH) calibration model (2001). In its original formulation, KOH formulates a hierarchical Bayesian framework with two sequential phases. In the first phase, model hyperparameters are predicted using maximum likelihood estimation (MLE) method. In the second phase, Bayesian analysis is used to derive the posterior distribution of calibration parameters while fixing the hyperparameters found in the first phase. Neither KOH's 
original formulation nor later suggestions are fully Bayesian as they use MLE methods to estimate hyperparameters, largely due to computational infeasibility (CITATION: Higdon, Berger, other). 

On the other hand, `FBC` runs a fully Bayesian model that includes both calibration parameters and model hyperparameters in its Bayesian framework. Implementation of the package optimizes the calibration process and memory management to increase computational efficiency. Moreover, the fully implemented Bayesian framework, enables the user to input the information about all parameters and hyperparameters in the  form of prior specification. Common prior distribution are implemented in 
`FBC` to allows for high degree of flexibility in specifying the expert knowledge or the lack thereof. `FBC` runs a Markov Chain Monte Carlo (MCMC) algorithm to find the posterior distribution of calibration parameters and model hyperparameters. Furthermore, `FBC` can be used for prediction of mean response and model discrepancy for a new test input configuration. 

(TODO: edit this paragraph to reflect the structure of the vignette in the end) The current vignette is structured into following sections: The first section (`FBC` Usage) explains the package functionality with a simple pedagogic example. Also in this section, the notation for inputs and outputs of both computer and physical experiments are introduced. The second section 
(Calibration Model) generalizes the example introduced in the first section to build model components. Using the general notation while referencing the example, we describe the KOH calibration model and its modelling choices. The third section (Parameter Estimation), presents the theoretical results that characterize the posterior distribution of model parameters and MCMC-based 
estimation of parameters. The fourth section (Prediction), provides the results to derive MCMC-based predictions for new input configuration using the estimated parameters and MCMC samples of model components. The fifth section (Implementation) explains some of the implementation features and choices that distinguishes `FBC` from other implementations while justifying those choices. Finally, the last section (Application) provides two real-world and well-cited examples to demonstrate the 
full functionality and limitations of the `FBC` package. In the end, an appendix is provided to provide further details and a reference page to link for further readings.



## 1. `FBC` Usage

`FBC` package has two main public functions: `calibrate()` and `predict()`. As the name suggests, `calibrate()` takes the field and simulation data to calibrate the simulator model and `predict` takes a new input configuration and the calibration model (`fbc` object) to predict the field response at new input configuration and quantifies the uncertainty in prediction. In addition, `FBC` has a few helper function to aid in arguments entry and visualization.


### 1.1 Setup

Building a calibration model requires data from field experiment and computer simulation. To focus on the functionality of the package, we use a simple pedagogic example as experimental setting. In this setting, a wiffle ball is dropped from different heights and the time it takes to hit the ground is measured. This experiment has one experimental input height ($h$) and one calibration input gravity ($g$) to produce the response, time ($t$). Note that in the field experiment, the 
earth gravity is fixed but unknown to the experimenter (at least to some level of uncertainty) and therefore it is not part of the input data. The field experiment with aforementioned specification has been performed by Derek Bingham and Jason Loeppky (CITATION). The data field is loaded in the package environment under `ballField` name. To increase robustness, `calibrate()` requires the training field data to include both field response vector $\bf{t}$ and input matrix $\bf{h}$ (a vector in ball example) in a single input matrix `ballField`, where first column always contain the response, followed by experimental input variables as columns in the field data matrix.


```{r ballField, echo=TRUE}
library(FBC)
head(ballField, 3)
dim(ballField)
```


Simulation of the ball drop experiment is easy to model using introductory physics results: 


$$
t = \sqrt{\frac{2h}{g}}
$$


We have implemented the above mathematical model as a code that takes $h$ and $g$ as experimental and calibration inputs and returns $t$ as simulator response. Latin Hypercube Design (CITATION) is used to create the input design matrix consisting of two columns: $h$ and $g$. Similar to the field data, `calibrate` requires the simulation data to be packaged into a single matrix `ballSim`, where first column always represents response vector ($\bf{t}$), followed by experimental and calibration inputs as columns $[\bf{h} \quad \bf{g}]$.  


```{r ballSim, echo=TRUE}
head(ballSim, 3)
dim(ballSim)
```


Figure 1 shows the distribution of time versus height for both physical and simulation experiments. Note that for higher height values, the simulation responses (blue) underestimate their corresponding field response (red), displaying a systemic bias for simulation model. 



```{r ball plot, echo=FALSE, fig.height=5, fig.width=7}
plot(ballField[, 2], ballField[, 1], cex = 0.65, pch = 19, col = "red", ylim = c(0, 1.5), 
     xlab = "Height (m)", ylab = "Time (s)")
points(ballSim[, 2], ballSim[, 1], cex = 0.65, pch = 19, col = "blue")
legend("topleft", legend = c("Simulation", "Field"), 
       col = c("blue", "red"), pch = 16, cex = 0.8)
title(main = "Figure 1: Ball Drop Experiments", adj = 0)
```



The ball example is a suitable pedagogical example as it has only one experimental input $\bf{h}$) and one calibration input $\bf{g}$ to model response $\bf{t}$. Using this toy example, we demonstrate the functionality of the package without getting to the details of a complex mathematical model. More complex and real-world examples are covered in the last section. 


### 1.2 \ \ `calibrate()` \ \ 


**Arguments:** The `calibrate()` function takes three sets of arguments from user: data, MCMC, and prior specification parameters. Other than data arguments which must be supplied by user, all other arguments have reasonable default values for ball example.

```{r calibrate, eval=FALSE}
output <- calibrate(sim = ballSim, field = ballField,                                 # Data 
                    nMCMC = 11000, nBurn = 1000, thinning = 50,                       # MCMC
                    kappaDist = "beta", kappaInit = NA, kappaP1 = 1.1, kappaP2 = 1.1, # Priors
                    hypers = set_hyperPriors(),
                    showProgress = FALSE) 
```


The first and second arguments `sim` and `field` must be supplied by user. Both must be either in matrix or dataframe format representing the simulation and field data respectively. In both `ballSim` and `ballField` of our ball example, the first column represent the response $\bf{t}$ and the second column represent experimental input $\bf{h}$. Additionally, `ballSim` has a third column that represents calibration input $\bf{g}$. Calibration inputs are implicit in field experiment and are absent in the data matrix `ballField`. 


$$
\begin{aligned}
&\quad \quad \text{Response}  \quad  \text{Experimental} \quad \text{Calibration} \\
\texttt{ballSim} \quad &= \
\begin{bmatrix}
\quad \quad \bf{t} & \quad \quad \quad \quad \bf{h} & \quad \quad \quad \quad \quad \bf{g} \quad \quad   
\end{bmatrix} \\ \\
\texttt{ballField} &= \
\begin{bmatrix}
\quad \quad \bf{t} & \quad \quad \quad \quad \bf{h} & \quad \quad  
\end{bmatrix}
\end{aligned}
$$

The second set of arguments consists of MCMC parameters such as number of total iterations `nMCMC`, number of burn-in iterations to be removed from the beginning of the chain `nBurn`, and `thinning`, which indicate the sampling rate to remove the autocorrelation from the sampled draws. For example, when `thinning = 50`, for every 50 draws from the result only one will be kept in order to remove the correlation between draws. 


The third and last set of arguments consists of prior specification for each parameter of the model. The main goal of calibration is to estimate the calibration parameters ($g$ in ball example). However, employing KOH calibration model introduces seven additional classes of hyperparameters to the model. All eight classes of parameters are unknown and must be estimated. As `FBC` employs a full Bayesian framework, the priors for all model parameters must be specified in advance. Throughout the package implementation and current guide a consistent notation is used to denote parameters: $\bf{\kappa}$ (calibration parameters), $\bf{\theta_s}$ (simulator correlation scale parameters), $\bf{\alpha_s}$ (simulator correlation smoothness parameters), $\bf{\theta_b}$ (bias-correction correlation scale parameters), $\bf{\alpha_b}$ (bias-correction correlation smoothness parameters), $\sigma^2_s$ (marginal simulator variance), $\sigma^2_b$ (marginal bias-correction variance), $\sigma^2_{\epsilon}$ (measurement error variance), and $\mu_b$ (mean of bias-correction Gaussian Process. All calibration model components along with the introduced parameters are explained in more detail in next section. 

For each class of parameters, there are four associated arguments. 1) distribution (suffixed with "dist" in argument names) type which is a character string determining the prior distribution. Currently, `FBC` supports almost all common distributions and can be chosen from ("uniform", "gaussian", "gamma", "beta", "lognormal", "logistic", "betashift", "exponential", "inversegamma", "jeffreys", "fixed"). "betashift" refers to a beta distribution that is shifted one unit to right to cover [1 2] interval and it is used to specify the priors for correlation smoothness parameters which must be constrained to [1 2] interval. Moreover, choosing "fixed" as distribution will exclude that class of parameters from the MCMC sampling. In this case, the given initial value will be used as fixed parameter value and p1 and p2 arguments are not used. 2) Initial value (suffixed with "init" in argument names) which represent starting point of the parameter(s) in MCMC algorithm. 3) and 4) represent the two parameters (p1 and p2) of the chosen distribution. For example, if "gaussian" distribution is used, p1 and p2 represent mean and variance of the distribution and if "uniform" distribution is used, p1 and p2 represent lower and upper bound of the distribution. Please note that not all distribution types require two arguments. In particular, "exponential" distribution only requires tone parameter (p1) and "jeffreys" requires none. In these cases the unused arguments are ignored.. 


From the classes of parameters mentioned, the prior for calibration parameters must be specified by user based on field knowledge as these parameters are problem-specific, even though a general vague prior is specified as default values. In contrast, all other parameters have reasonable default values based on literature and can be left unchanged. For this reason and to avoid unwanted complexity, all other priors are specified using a helper function `set_hyperPriors()`. Using this function without argument (which is the default value of the `hyper` argument) will specify all parameters based on their default values. However, expert knowledge about one or more of these parameters can be supplied using arguments of the `set_hyperPriors()` function to change the default prior specifications. The following table summarizes all prior arguments to build the calibration model. 

\rule{\linewidth}{0.75pt}
**Table 1:** *Argument names to specify priors for each parameter class.*

|Parameter Class                                   |Distribution |Initial Value|Shape Parameters         |
|:-------------                                    |:------      |:-----       |:-----------             |
|True field calibration inputs ($\kappa$)          |`kappaDist`  |`kappaInit`  |`kappaP1`  &  `kappaP2`  |
|                                                  |             |             |                         |
|Simulation GP scale ($\theta_s$)                  |`thetaSDist` |`thetaSInit` |`thetaSP1` & `thetaSP2`  |
|                                                  |             |             |                         |
|Simulation GP smoothness ($\alpha_s$)             |`alphaSDist` |`alphaSInit` |`alphaSP1` & `alphaSP2`  |
|                                                  |             |             |                         | 
|Bias-correction GP scale ($\theta_b$)             |`thetaBDist` |`thetaBInit` |`thetaBP1` & `thetaBP2`  | 
|                                                  |             |             |                         |
|Bias-correction GP smoothness ($\alpha_b$)        |`alphaBDist` |`alphaBInit` |`alphaBP1` &`alphaBP2`   | 
|                                                  |             |             |                         |
|Simulation marginal variance ($\sigma^2_s$)       |`sigma2SDist`|`sigma2SInit`|`sigma2SP1` & `sigma2SP2`|
|                                                  |             |             |                         | 
|Bias-correction marginal variance ($\sigma^2_b$)  |`sigma2BDist`|`sigma2BInit`|`sigma2BP1` & `sigma2BP2`|
|                                                  |             |             |                         | 
|Measurement error variance ($\sigma^2_{\epsilon}$)|`sigma2EDist`|`sigma2EInit`|`sigma2EP1` & `sigma2EP2`| 
|                                                  |             |             |                         | 
|Bias-correction mean ($\mu_b$)                    |`muBDist`    |`muBInit`    |`muBP1`    & `muBP2`     |


Distribution arguments must be chosen from the string list of implemented prior distributions and initial value and the two shape parameter arguments must be numeric. Note that some classes of parameters may contain more than one parameters. In this case, the argument values can be vector instead of default scaler. In this case all four fields of that parameter class must be in vector
format with same length as number of parameters in the same class. For example if there are five calibration parameters, `kappaDist` can either be a scaler, in which case the distribution for all calibration parameters will be set to that scaler value (and `kappaInit`, `kappaP1`, and `kappaP2` must be also scaler), or a vector of length five that supplies the distribution types for all calibration parameters (and `kappaInit`, `kappaP1`, and `kappaP2` must also be vector of length 5).

Moreover, there is a logical argument `showProgress` that indicate whether function must show the progress in calibration on console. This will put the `calibrate()` in interactive mode and will show the percentage of the MCMC draws along with sample draws. 

**Output:** The output of the `calibrate()` function is a `fbc` object that contains the samples from posterior joint distribution of parameters, along with other model information.


```{r load output, include=FALSE}
output_path <- system.file("output.rda", package = "FBC", mustWork = TRUE)
load(output_path)
```


```{r output}
names(output)
```


The first and main component of the output is matrix `Phi` whose columns represent the sample of posterior densities for each unknown parameter of the model in the same order as parameter classes in table 1. Since $\kappa$, $\theta_s$, $\alpha_s$, $\theta_b$, $\alpha_b$ parameter classes may contain more than one parameter, they are suffixed by a number that represents the index of the parameter in the class. For example, if there are 3 calibration inputs, the first $p$ columns of the matrix `Phi` represent posterior density of calibration parameters and the column headers will be `kappa1`, `kappa2`, and `kappa3`. In the ball example, there is only one calibration parameter $g$, which is denoted by $\kappa_1$ and represented by `kappa1` in matrix `Phi`. Each row of the matrix `Phi` represents a MCMC draw.


```{r Phi columns}
head(output$Phi, 3)
```


All other columns of matrix `Phi` represent the samples for other model hyperparameters. Table 2 provides an overview of the model parameters and the notation to represent them in ball example. Later sections will explain in detail why are these hyperparameters introduced, what do they represent, and how they are estimated.


**Table 2:** Notation used in matrix `Phi` to represent parameters in ball example.

|Column   |Notation             |Description                                                       |
|:---     |:---                 |:-------------------                                              |
|`kappa1` |$\kappa_1$           |Unknown value of true calibration input $g$                       |
|         |                     |                                                                  |
|`thetaS1`|$\theta_{s1}$        |Scale parameter of $h$ input for simulator correlation            |
|         |                     |                                                                  |
|`thetaS2`|$\theta_{s2}$        |Scale parameter of $g$ input for simulator correlation            |
|         |                     |                                                                  |
|`alphaS1`|$\alpha_{s1}$        |Smoothness parameter of $h$ input for simulator correlation       |
|         |                     |                                                                  |
|`alphaS2`|$\alpha_{s2}$        |Smoothness parameter of $g$ input for simulator correlation       |
|         |                     |                                                                  |
|`thetaB1`|$\theta_{b1}$        |Scale parameter of $h$ input for bias-correction correlation      |
|         |                     |                                                                  |
|`alphaB1`|$\alpha_{b1}$        |Smoothness parameter of $h$ input for bias-correction correlation |
|         |                     |                                                                  |
|`sigma2S`|$\sigma^2_s$         |Marginal variance of simulator covariance                         |
|         |                     |                                                                  |
|`sigma2B`|$\sigma^2_b$         |Marginal variance of bias-correction covariance                   |
|         |                     |                                                                  |
|`sigma2E`|$\sigma^2_{\epsilon}$|Variance of random measurement error in field                     |
|         |                     |                                                                  |
|`muB`    |$\mu_b$              |bias-correction mean                                              |



`estimates` is a data frame that provides a summary table of all model parameters. `summary()` function also provides the same statistics given a `fbc` object. `logPost` is vector of computed posterior log likelihood given a parameter draw from `Phi` matrix. Therefore, the length of the `logPost` vector is same as number of rows in `Phi`. `priors` is a nested list that contains prior specifications for all parameters (including calibration parameters). `acceptance` is a vector of same length as number of unknown parameters and represent the acceptance rate of each parameter in MCMC algorithm. The current implementation of MCMC algorithm employs Metropolis-Within-Gibbs variation, which is a one-dimensional proposal scheme and the optimal acceptance rate must be close to 0.44. `vars` is a vector of character string with the same length as number of parameters and contains the parameter notation used in code and as column headers. `data` is a list consisting of training data in the form of vectors and matrices. `scale` is a numeric vector that contains scaling factors used during calibration to scale the training data. `indices` is a list of vectors containing the indices of parameters in each row of `Phi` matrix. `priorFns` is a list of prior functions that are created during calibration based on given prior specifications. And finally, `proposalSD` is a numeric vector that represents the final standard deviation of proposal for each parameter.



### 1.3 \ \ `predict()`

**Arguments:** Similar to any other predict function, `predict()` requires a model object argument called `object`, which in `FBC` package must be a `fbc` object, along with an argument representing a new input configuration called `newdata`. Moreover, current implementation of the `predict()`, support to different methods of prediction: Maximum A Posteriori ("MAP") and MCMC-based fully Bayesian ("Bayesian") methods. The method can be selected using `method` argument that can take a
character string value of either "MAP" or "Bayesian".

```{r predict}
predsMAP   <- predict(object = output, newdata = matrix(c(2.2, 2.4), ncol = 1), method = "MAP")
predsBayes <- predict(object = output, newdata = matrix(c(2.2, 2.4), ncol = 1), method = "Bayesian")
```


**Output:** `predict()` returns a list consisting of two fields: `pred`, which is a vector of the predicted response ($\hat{t}$ in ball example) with same length as `newdata` rows, and `se`, which is a vector of the predicted response's standard errors, again with the same length as `newdata` rows.

```{r}
predsMAP  
predsBayes
```

In "Bayesian" method, which is the default method, MCMC draws of calibration model parameters are used to form a distribution of each predicted value. In particular, each rows of matrix `Phi` from the output of the calibration model, is used to predict the response for a single row (a new input configuration) of `newdata`. Therefore, for each observation a MCMC-based distribution of predicted responses are computed. Then, the predictive mean and variance of this  distribution are used to computed point predictions as well as standard errors for predictions.

In "MAP" method, the row of `Phi` matrix that results in maximum log posterior, is extracted and taken as model parameters to compute both point estimates and standard errors. This method is much faster than the "Bayesian" method as it only computes the prediction for each row of observation once. More detail on theory behind prediction methods can be found in section  


### 1.4 \ \ `set_hyperPriors()`:

As mentioned hyperparameters of the calibration model can be set using this function. All hyperparameters have reasonable default values. Therefore `set_hyperParameters()` can be used without arguments to set the hyperparameters. In fact, the default value of the `hyper` argument in `calibrate()` is the function `set_hyperParameters()` without any argument. Nevertheless, when there is prior belief about structure of correlation structures (either simulator GP or bias-correction GP), these beliefs can be applied to the model in the form of prior specification using `set_hyperPriors()`

```{r set_hyperPriors}
priors <- set_hyperPriors(thetaSDist = "beta", thetaBP2 = 6)
```



### 1.5 \ \ `summary()` and `print()`:

Both of these generic functions are implemented to work with the output of `calibrate()` function. In particular, given a `fbc` object, `summary()` displays the summary data frame of calibration parameters which is the `estimate` component of `calibrate()` output. Similarly, `print()` will print the data frame in the console.

```{r summary and print}
summary(output) # print(output) has exactly the same output
```


### 1.6 \ \ `plot()`

As name suggests, the implementation of this generic function enables visualization of a calibration model results. Given a calibration model in the form of a `fbc` object, `plot()` can visualize the model in three different mode, which can be chosen by supplying the `type` argument. In particular, `type` must be a character string form "density", "trace", and "fits". 

Using "density", which is the default value of the `type` argument, `plot()` will plot the marginal posterior density distribution for the given parameter. It does so by estimating a density function given the MCMC-based posterior draws of the parameter. It also plots the prior distribution of the given parameter in the same plot for ease of comparison and visualizes the empirical mode of the posterior density. The given parameter must also be given as character string sing the `parameter` argument. The parameter name is consistent with the notation used throughout the package and current vignette and must be chosen from "kappa", "thetaS", "alphaS", "thetaB", "alphaB", "sigma2S", "sigma2B","sigma2E", or "muB". The default value for `parameter` argument is "kappa" (calibration parameters), which usually is the parameter of the interest. Note that `parameter` argument characterizes the class of parameters and when there is more than one parameter in that class,`plot()` will plot
the density distribution for all parameters in that class in separate plots.


```{r plot density}
# plot other model parameters (note that since there are two correlation scale parameters for
# simulator GP, there will be two plots)
plot(output, parameter = "thetaS")
```


Using "trace" as `type` argument, `plot()` will plot the progression of the given parameter as MCMC draws are taken. It is similar to the time series of the parameter but indexed with number of iteration in MCMC rather than time. The trace plot can be used to determine whether there is good mixing in MCMC draws. Using "trace" as `type` argument also requires supplying the `parameter` from aforementioned list of possible parameter classes. And similar to density plots, trace plots will be plotted for all of the parameters in the given class in separate plots. 

```{r plot trace}
# plot the trace of MCMC draws (note that this plot is useful to determine convergence and
# stationarity of the parameter draws. This minimal example dhas only few draws!)
plot(output, parameter = "alphaS", type = "trace")
```



And finally using "fits" as `type` argument, `plot()` will plot the fitted values of the response versus all experimental variables in separate plots. The `parameter` argument is not required for this type and will be ignored. Fits plots can be used to visually determine the goodness of fits versus actual response in interpolations. Internally, `plot()` will use `predict()` function (using "MAP" method) to compute the fitted values for the training input configurations and will plot them in conjunct with actual values. Furthermore, `xlab` argument can be supplied with a character string to characterize the experimental variables' names.


```{r plot fits}
# plot the fitted values of the calibration model. It will plot the fitted values versus all
# experimental inputs, in this example only one!
plot(output, type = "fits", xlab = "height")
```

There are also three more exported functions in `FBC` package that are not required to build calibration models or to
predict based on calibration models. These functions are used internally in the package but since they offer functionalists that are not supported in base R, they are exported for use.

### 1.7 \ \ `correlation()`:

This function is used to compute the correlation between rows of two given matrices assuming a power exponential correlation family structure. This family is a generalization of correlation where distinct scale and smoothness hyperparameters are used for different dimensions of the given data (columns of given matrices) when computing the correlation. A more thorough definition of power exponential correlation family is explained in section two. `correlation()` function is more general than R's base correlation function `cor()`, as it treats different dimensions using separate scale parameters and also allows for use of smoothness parameters (also for different dimensions) when computing the correlation. Note that only the first matrix, characterized by argument `X` must be supplied and the default value for the second matrix, characterized by `Y` is `NULL`. When only a single matrix is supplied, `correlation()` will compute the correlation of that matrix with itself. Other than the given matrix or matrices, which must have same number of columns, user must supply two vectors with `theta` and `alpha` arguments to characterize scale and smoothness parameters. The length of both vectors either must be same and equal to the number of columns in given matrices, or they must be scaler in which case for all dimensions same values of scale or smoothness will be used.


```{r correlation}
X     <- matrix(c(1, 3, 5,
                  2, 2, 6,
                  1, 4, 1), nrow = 3, byrow = TRUE)

Y     <- matrix(c(7, 3, 0,
                  2, 2, 4), nrow = 2, byrow = TRUE)

sc    <- c(1, 2, 3) # scale parameters of correlation structure
sm    <- c(2, 1, 2) # smoothness parameters of correlation structure

# correlation of a matrix with itself
round(correlation(X, theta = sc, alpha = sm), 5)

# correlation between two matrices
round(correlation(X, Y, theta = sc, alpha = sm), 5)
```



### 1.6 \ \ `prior_builder()`

This function will create a prior function based on given distribution and distribution parameters. The function has three arguments: `prior` takes a character string from ("uniform", "gaussian", "gamma", "beta", "lognormal", "logistic", "betashift", "exponential", "inversegamma", "jeffreys", "fixed") and characterizes the type of distribution. When `fixed` is used, the prior function is simply `x = 1`. The next two arguments, `p1` and `p2` characterize the parameters of distribution. For example for gamma distribution `p1` and `p2` determine shape and scale of the gamma distribution, or for Gaussian distribution, `p1` and `p2` determine mean and standard deviation of the Gaussian distribution. The output of `prior_builder()` is a function that given a value, computes the probability density of the chosen distribution. To reduce the load of internal computation, `prior_builder()` computes the log of density and if used externally must be transformed.

```{r  prior_builder}
# create a prior function for beta(2, 5). Note that the function compute log of priors and must be transformed
pr_fun <- prior_builder(prior = "beta", p1 = 2, p2 = 5)
round(exp(pr_fun(c(-1, 0, 0.1, 0.5, 0.9, 1, 2))), 3)

# create a prior function for a Uniform distribution with lower bound of -10, and upper bound of 10
pr_fun <- prior_builder(prior = "uniform", p1 = -10, p2 = 10)
round(exp(pr_fun(c(-11, -5, 0, 4, 10, 12))), 3)

# create a prior function for Gaussian distribution with mean of 1 and standard deviation of 2
pr_fun <- prior_builder(prior = "gaussian", p1 = 1, p2 = 2)
round(exp(pr_fun(c(-9, -5, -3, -1, 1, 3, 5, 7, 11))), 3)
```


### 1.7 \ \ `pmode()`

This function computes an estimate of the mode of a continuous distribution. It works similar to a histogram, in which the domain of the distribution is broken into same length bins. For each bin, the draws of the distribution that fall within that bin is counted and at the end the mean of the bin with highest count is returned as estimated mode. The function also takes the number of bins through `breaks` argument. The default value is `NULL`, which makes `pmode` to determine the number of required bins dynamically based on number of data points and the domain of the distribution.

```{r pmode}
# find the estimated mode of a vector
vec <- runif(100, 0, 10)
pmode(vec) 
pmode(vec, breaks = 10)
```


## 2. Calibration Model

Calibration model is statistical model that represent both field and simulator response as function of input configuration. In this section, we explain the theory behind building a calibration model and relate the notation used in the formulation of calibration model and in the package implementation to our ball example.


### 2.1 Data

A computer experiment or simulation is simply running a computer code at different input configurations and recording the response. The code is an implementation of the mathematical model that is intended to mimic the physical experiment. In general, a simulation has $p$ experimental inputs but also has $q$ additional calibration inputs that are either tuning parameters or unknown physical properties that are not controllable by field experimenter. Table 2 describes the response and inputs of a computer experiment with $m$ observations using matrix notation. The subscript $s$ is used to denote simulation.


\rule{\linewidth}{0.75pt}
**Table 2:** Notation used to represent simulation data component of calibration model.

| ***Notation*** | ***Description***                                 | ***Ball Example***                            |
| ---            | ---------------                                   | ------                                        |
| $m$            | Number of simulation runs                         | 100                                           |
|                |                                                   |                                               |
| $p$            | Number of experimental inputs                     | 1                                             |
|                |                                                   |                                               |
| $q$            | Number of calibration inputs                      | 1                                             |
|                |                                                   |                                               |
| $\bf{x}_s$     | Simulation input vector containing $(p+q)$ inputs | ($h$, $g$) (vector of length 2)               |
|                |                                                   |                                               |
| $y_s$          | Univariate simulation response                    | $t$ (scaler)                                  |
|                |                                                   |                                               |
| $\bf{X}_s$     | $(m \times (p+q))$ simulation input matrix        | [$\bf{h}$ $\bf{g}$] ($(100 \times 2)$ matrix) |
|                |                                                   |                                               |
| $\bf{y}_s$     | Vector of $m$ univariate simulation response      | $\bf{t}$ (vector of length $100$)             |



On the other hand, a physical experiment consists of $n$ observations of a physical property, each with $p$ experimental inputs. The calibration inputs are implicit in physical experiment and their values are unknown but assumed to be fixed throughout observations. To represent both experiments in a unified structure, the unknown calibration inputs of field experiment $\kappa$ must be augmented to experimental inputs, so that both physical and computer experiments have $(p+q)$ inputs ($h$ and $g$ in ball example) and a univariate response ($t$ in ball example). Assuming vector $\bf{x}_f$ represent experimental input, the augmented input vector is denoted by $\bf{x}_{\kappa}$. Since $\bf{x}_f$ has $p$ elements and $\bf{\kappa}$ has $q$ elements, both will have $(p+q)$ elements similar to $\bf{x}_s$. Stacking the input vectors, we can represent all field and augmented input vectors using $\bf{X_f}$ and $\bf{X_{\kappa}}$. Similarly, vector $\bf{y_f}$ represents all field observations. Subscripts $f$ and $\kappa$are used to denote field and augmented data respectively. 

Elements of vector $\kappa$ are parameters of the calibration model and will be estimated by `calibrate()` [^2]. 

\

[^2]: *The first $q$ columns of matrix `Phi` in `calibrate` output represent the MCMC samples of posterior densities for calibration parameters. In our ball example, there is only one calibration parameter (gravity), which is denoted by $\kappa_1$ and represented by `kappa1` in `Phi`.*</em>.

\pagebreak

\rule{\linewidth}{0.75pt}
**Table 3:** Notation used to represent field data component of calibration model.

| ***Notation***    | ***Description***                                                | ***Ball Example***                         |
| ---               | ---------------                                                  | -------                                    |
| $n$               | Number of field observations                                     | 63                                         |
|                   |                                                                  |                                            |
| $p$               | Number of experimental inputs                                    | 1                                          |
|                   |                                                                  |                                            |
| $\bf{x}_f$        | Field input vector containing $p$ experimental inputs            | $h$ (scaler)[^2]                          |
|                   |                                                                  |                                            |
| $\kappa$          | Vector of unknown true calibration inputs in field experiment    | true value of gravity $\kappa_1$           |
|                   |                                                                  |                                            |
| $\bf{x}_{\kappa}$ | Augmented field input vector containing $(p+q)$ inputs[^3]       | ($h$, $\kappa_1$)$^2$ (vector of length 2) |
|                   |                                                                  |                                            |
| $y_f$             | Univariate field response                                        | $t$ (scaler)                               |
|                   |                                                                  |                                            |
| $\bf{X}_f$        | $(n \times p)$ field input matrix                                | $\bf{h}$ (vector$^2$ of length $63$)       |
|                   |                                                                  |                                            |
| $\bf{X}_{\kappa}$ | $(n \times (p+q))$ augmented field input matrix                  | $[\bf{h} \quad \bf{\kappa_1}]$[^4]              |
|                   |                                                                  |                                            |
| $\bf{y}_f$        | Vector of $n$ univariate field response                          | $\bf{t}$ (vector of length $63$)           |
|                   |                                                                  |                                            |

[^2]: *In ball example there is only one experimental input and therefore $\bf{x}_f$ is a vector of length one or scaler. Similarly, in its matrix notation, $\bf{X}_f$ is a $(n \times 1)$ matrix or a vector of length $n$.*</em>.
[^3]: *Note that calibration parameters $\bf{\kappa}$ are often assumed to be unchanged throughout field experiment. Therefore, same ($\kappa$) vector is augmented to all of the field input configurations.*</em>.
[^4]: *Since calibration input is fixed for all field observation, $\bf{\kappa_1} = (\kappa_1, ... , \kappa_1)$ vector of length 63} ] ($(100 \times 2)$ matrix).*</em>.



All of the data components in Table 2 and 3 are generated internally to build the calibration model. The user is only required to provide two matrices representing the field and simulation datasets in their entirety through `sim` and `field` arguments of `calibrate()`. 


### 2.2 KOH Model 

#### Random Functions:
KOH models the functional relationship between simulation input and output as a realization of a random function $\eta(\bf{x_s})$. Similarly, KOH models the functional relationship between field input and output as a realization of random function $\eta(\bf{x}_{\kappa})$ but acknowledges a systemic model discrepancy and measurement errors. As a result, KOH models the discrepancy by adding a bias-correction term as realization of another random function $\delta_{\kappa}(\bf{x}_f)$. The error term $\epsilon$ is considered to be an independent draw from a normal distribution with zero mean and unknown variance $\sigma^2_{\epsilon}$. 


$$
\begin{aligned}
\epsilon  \ \ &\sim  \mathcal{N}(0, \sigma^2_{\epsilon}) \\ \\
y_f           &=     \eta(\bf{x_\kappa}) + \delta_{\kappa}(\bf{x_f}) + \epsilon \\ \\
y_s           &=     \eta (\bf{x_s})  \\
\end{aligned}
$$

Therefore other than $\bf{\kappa}$ and $\sigma^2_{\epsilon}$ parameters, the random functions $\eta(.)$ and $\delta_{\kappa}(.)$ are also unknown and must be specified. KOH models $\eta(.)$ and $\delta_{\kappa}(.)$  by two independent Gaussian Processes (GP). 


$$
\begin{aligned}
\eta(.) \  &\sim GP \ (0, \ \sigma^2_s . R_s(., .)) \\
\delta_{\kappa}(.) &\sim GP \ (0, \ \sigma^2_b . R_b(., .)) \\ \\
\end{aligned}
$$


Where $\sigma^2_s$ and $\sigma^2_b$ are marginal variance of simulator and bias-correction GPs, and $R_s(., .))$ and $R_b(., .))$ are correlation matrix of simulator GP (using full input matrix $\bf{X_s}$ or $\bf{X_{\kappa}}$) and bias-correction GP (using field input matrix $X_f$).

Note that means of GPs are considered to be zero because `calibrate()` function first standardizes simulation response $\bf{y}_s$ (mean zero and standard deviation of one) and then scales field response according to $\bf{y}_s$'s scaling factors. Furthermore, the simulator inputs (both experimental and calibration) are scaled to span $[0, 1]$ and the scaling factors of simulation experimental inputs are used to scale field experimental inputs. As a result considering zero mean for both processes seem reasonable. (TODO: look into the effect of constant mean for discrepancy GP).

#### Correlation Structure:
`FBC` employs a power exponential correlation family to represent the correlation structure of both GPs. Assuming $\bf{x}$ and $\bf{x'}$ are two rows of full input matrix (either $\bf{X_s}$ or $\bf{X_{\kappa}}$) and $\bf{x_f}$ and $\bf{x_f'}$ are two rows of field experimental input matrix ($\bf{X_f}$), the correlation matrices $R_s(\bf{x}, \bf{x'})$ and $R_b(\bf{x_f}, \bf{x_f'})$ are defined as following:


$$
\begin{aligned}
R_s(\bf{x}, \ \bf{x}') \ &= \prod^{p+q}_{i=1} e^{-\theta_i |x_i - x_i'|^{\alpha_i}} \\ \\
R_b(\bf{x_f}, \ \bf{x_f}')       &= \prod^{p}_{j=1} e^{-\theta_j |x_j - x_j'|^{\alpha_j}} \\
\end{aligned}
$$


Using separable power exponential correlation family introduces two new hyperparameters for each input: scale ($\theta_i$) and smoothness ($\alpha_i$). Together, they flexibly determine the shape of correlation structure. Table 4 introduce the notation used for hyperparameters of $\eta(.)$ and $\delta_{\kappa}(.)$.


\rule{\linewidth}{0.75pt}
**Table 4:** *New Parameters To Build KOH Calibration Model.*

|**GP**              | **Hyperparameters**                                                          | **Ball Example (Columns of `Phi`)**                  |
| ---                | ----------                                                                   | --------------                                      |
|                    |                                                                              |                                                     |
| $\eta(.)$          |$(\theta_{s1},...,\theta_{s(p+q)},\alpha_{s1},...,\alpha_{s(p+q)},\sigma^2_s)$|(`thetaS1`, `thetaS2`,`alphaS1`,`alphaS2`, `sigma2S`)|
|                    |                                                                              |                                                     |
|$\delta_{\kappa}(.)$| $(\theta_{b1},...,\theta_{bp},\alpha_{b1},...,\alpha_{bp},\sigma^2_b)$       | (`thetaB1`, `alphaB1`, `sigma2B`)                   |


#### Full Model:
After augmentation of true calibration inputs (vector $\bf{\kappa}$ to field data, both simulation and field experiment have the same input. KOH combines both components to build a joint model. The joint vector of all parameters in the final calibration model is denoted by:


$$
\phi = (\kappa_1, \ ... , \ \kappa_q, \ 
\theta_{s1}, \ ... , \ \theta_{s(p+q)}, \
\alpha_{s1}, \ ... , \ \alpha_{s(p+q)}, \
\theta_{b1}, \ ... , \ \theta_{bp}, \
\alpha_{b1}, \ ... , \ \alpha_{bp}, \ 
\sigma^2_s, \ \sigma^2_b, \ \sigma^2_{\epsilon})
$$


Note that in the ball example, the column headers of matrix `Phi` exactly match to model parameters.


```{r}
#head(output$Phi, 3)
```


### 2.3 Model Parameters

Table 5 provides a general overview of all model parameters, the notations, and corresponding parameters in the ball example.


**Table 5:** General notation used to represent model parameters and an example of corresponding identifiers in `Phi` matrix.

| **Parameter**                     | **General Notation**                               | **Ball Example**      | 
| --------------                    | --------                                           | -------------         |
| True field calibration inputs     | $\kappa = (\kappa_1, ... , \kappa_q)$              | (`kappa1`)            | 
|                                   |                                                    |                       | 
| Simulation GP scale               | $\theta_s = (\theta_{s,1}, ... , \theta_{s,p+q})$  | (`thetaS1` `thetaS2`) |  
|                                   |                                                    |                       | 
| Simulation GP smoothness          | $\alpha_s = (\alpha_{s,1}, ... , \alpha_{s,p+q})$  | (`alphaS1` `alphaS2`) |  
|                                   |                                                    |                       | 
| Bias-correction GP scale          | $\theta_b = (\theta_{b,1}, ... , \theta_{b,p})$    | (`thetaB1`)           |  
|                                   |                                                    |                       | 
| Bias-correction GP smoothness     | $\alpha_b = (\alpha_{b,1}, ... , \alpha_{b,p})$    | (`alphaB1`)           | 
|                                   |                                                    |                       | 
| Simulation marginal variance      | $\sigma^2_s$                                       | `sigma2S`             |
|                                   |                                                    |                       | 
| Bias-correction marginal variance | $\sigma^2_b$                                       | `sigma2B`             |
|                                   |                                                    |                       | 
| Measurement error variance        | $\sigma^2_{\epsilon}$                              | `sigma2E`             |  



Each row of matrix `Phi` represents a draw from joint distribution of parameters (MCMC run) and each column represents a parameter in the model. User-given initial values for parameters are used to initialize the first row of the `Phi` matrix. Then, each row will be used to find another sample from joint parameter space to fill the next row of `Phi` until matrix `Phi` is complete.


## 3. Parameter Estimation

`FBC` employs a full Bayesian approach to jointly estimate all parameters. To find the marginal posterior density distribution for each parameter, we need prior specification for each parameter (prior knowledge) and the joint likelihood estimation (full data).


### 3.1 Bayesian Analysis


Because `FBC` uses a full Bayesian framework, expert knowledge or opinion can be applied to the model parameters as prior specification. Variety of common prior distributions are implemented in `FBC` and can be used to specify the priors for each parameter (see section 1.2). There are seven classes of parameters and all have been specified in `calibrate()` using default values. Of those
seven classes, calibration parameters (vector $\bf{\kappa}$) and perhaps measurement error variance (scaler $\sigma^2_{\epsilon}$) are application-dependent. It is recommended for user to specify the prior arguments for these parameters based on prior knowledge or consensus. Nevertheless, prior for calibration parameters is defaulted to Beta(1.1, 1.1) distribution [^5]. It is close to standard uniform distribution (U(0, 1)) but densities approach to zero sharply as samples approach boundaries. This default choice has been made to ensure a somewhat non-informative prior while de-emphasizing on boundary values[^6]. For all other classes of parameters reasonable priors have been specified using default values. Priors for correlation scale parameters (vectors $\theta_s$ and $\theta_b$) have been set to Gamma(1.1, 0.1) distribution. Similarly, the priors for correlation smoothness parameters (vectors $\alpha_s$ and $\alpha_b$) have been set to Beta(5, 2) distribution that is shifted one unit to right to span [1, 2] as is the acceptable range for moothness parameters. This choice emphasizes higher (closer to 2 than 1) smoothness parameters. Finally, the priors for marginal simulator and  bias-correction and measurement error variances (scalers $\sigma^2_s$, $\sigma^2_b$, and $\sigma^2_{\epsilon}$) are set to be Inverse Gamma(1.5, 1.5). This emphasizes very low variances and de-emphasizes higher values.

[^5]: *Note that the range of Beta distribution is the span of [0, 1], which is the range of calibration inputs for simulator after scaling* </em>.
[^6]: *Boundary values of [0, 1] corresponds to $-\infty$ and $\infty$ in the original scale of calibration parameters.* </em>.



By representing both data in a joint calibration model, we can compute the conditional likelihood of response given a parameter vector
(See Appendix). Therefore, given the prior specifications above, we can derive joint posterior distribution of parameters given data:


$$
\mathcal{P} [\phi | \mathcal{D}]      \ \propto \ L(\mathcal{D} | \phi) \ .  \ \mathcal{P} [\phi]     
$$

Where $\mathcal{D}$ represent full data (field and simulation). However, the above formulation is intractable and thus we need a simulation-based method to sample from joint posterior distribution. `FBC` implements a version of Markov Chain Monte Carlo (MCMC) simulation.

### 3.2 MCMC Simulation

MCMC simulation algorithm is used to draw samples from joint posterior distribution of the parameter space and build `Phi` matrix 
row by row. MCMC algorithm creates a Markov chain by updating parameters in each iteration according to a proposal scheme. Then given this parameter vector ($\phi^{(i)}$) and data ($\mathcal{D}$), the posterior likelihood can be computed. If posterior probability density of the parameters is larger than the density of a random draw from standard uniform distribution, the algorithm keeps the that parameter configuration by writing the next row of matrix `Phi`, otherwise updates the new row by last parameter vector. In either case, the new row will be used to generate the next proposal. Note that the first row of `Phi`, which is needed to start the algorithm, is supplied by user through initial values for the parameters. The detailed algorithm is presented in the Appendix.



### 3.3 Parameter Posterior Distributions

At the end of the MCMC run, sample of the joint posterior distribution for each parameter (a column in matrix `Phi`) can be used as an approximation of marginal posterior distribution. Center measures such as mean, mode, or median are provided as parameter estimate depending on the application and distribution shape. Furthermore, 50% and 80% credible sets are formed for each parameters to quantify the uncertainty in estimation. These statistics are provided in `summary` element in the output of `calibrate()` and additionally. (TODO: fix estimate->summary)


```{r parameter estimates}
output$estimates
```

Alternatively, posterior density kernels can be visualized over their assumed prior to investigate the effect of data on priors for each parameter. 


```{r plots}
plot(output)
```


## 4. Prediction

As mentioned earlier, building a calibration model results in a `fbc` object, which contains matrix `Phi` as a MCMC-based sample from joint posterior distribution of all parameters of the model. This sample can be used to to predict the physical response, simulator response, and bias for new input configurations. in `FBC`, two methods have been implemented for prediction. In the first method `Bayesian`, which is the default value of `method` argument in `predict()` function, all joint samples (all rows of matrix `Phi`) are used to form a sample distribution for the new input configuration. Using the distribution, a point prediction is made along with the uncertainty associated with it. In the second method "MAP", the row of matrix `Phi` that results in maximum posterior log likelihood, is chosen to form the point prediction.


### 4.1 Setup:

The input data for the `predict()` function is taken by two arguments: `object` argument which takes a `fbc` object as calibration model and `newdata` argument which takes the new input configuration data in matrix format. Each row represents a new input configuration and columns represent experimental variables. It is similar to the `field` argument of the `calibrate()` function, except the first column in `field`, which is the physical response. `predict()` aims to predict the physical response for all row of `newdata` in the form of a vector of predicted responses and a vector of standard error. Both vectors will be the same length as number of rows in `newdata`. The third argument `method` characterized the method of prediction and must be chosen from one of the character string "Bayesian" and "MAP".

### 4.2 Bayesian Method

In the Bayesian method, the posterior distribution of the response, conditional on hyperparameter estimates (matrix `Phi`), is also a GP

$$
E(y_f(\bf{x_f}^*) | \phi) = \mu_b + \begin{bmatrix}
\bf{y}_f  \\
\bf{y}_s \\ 
\end{bmatrix}
$$


$$
\begin{aligned}
\bf{y} \  &=
 = 
(y_1, \ ... \ , \ y_{n+m})^T
\ \ &\text{(joint vector of responses)}  \\ \\
\bf{X}   &=
\begin{bmatrix}
\bf{X_{\kappa}}  \\
\bf{X_s} \\
\end{bmatrix} =
\begin{bmatrix}
\bf{x_1} \ \bf{x_2}  \ ... \ \bf{x_{p+q}} \\
\end{bmatrix}
\ \ &\text{(joint input matrix)}  \\ \\
\bf{x_i} &= (x_1, x_2, ..., x_{n+m}) \ \ \ \ \forall i \in \{1, 2, ..., p+q\} \ 
\end{aligned}
$$
### 4.3 MAP Method

## 5. Implementation


## 6. Application

### 6.1 Analytic Example:

### 6.2 Ball Example:

### 6.3 Spot Weld Example:

##### Description:

##### Experimental Input:

The physical model has three inputs: gauge ($G$), load ($L$), and current ($C$):

- Gauge ($G$):
- Load ($L$): 
- Current ($C$):


##### Calibration Input:

The simulation model has one additional input, $\tau$ that affects the amount of
heat produced in the metal sheets. $\tau$ cannot be controlled in the physical
experiment and its value is unknown. However it has to be specified for the 
simulation model as calibration input $t$:

- Heat generation factor $\tau$: Factor affecting amount of heat produced

##### Mapping Parameters:

To map the spot weld data (both field and simulation data) and parameters to 
`FBC` input configuration, we use the dagger $\dagger$ superscript to distinguish process
parameters and variables with `FBC` variables and parameters:

$$
\begin{aligned}
\text{x}_1 \quad &\longrightarrow \quad G^{\dagger} \\
\text{x}_2 \quad &\longrightarrow \quad L^{\dagger} \\
\text{x}_3 \quad &\longrightarrow \quad C^{\dagger} \\
\kappa_1 \quad &\longrightarrow \quad \tau^{\dagger}
\end{aligned}
$$



### 6.4 Kinetic Example


## Appendix

### A.1 MCMC Algorithm:

\rule{\linewidth}{1pt}
$$\textbf{Implementation of Metropolis within Gibbs algorithm}$$
\rule{\linewidth}{1pt}

Let $\Phi$ be the matrix of parameter values (columns) indexed by MCMC 
iterations. Each column represents (after MCMC completes) the posterior density
of a parameters. Since all parameters are included in $\Phi$ but have 
overlapping indices, the parameter densities (columns) are renamed to 
$(\phi_1, ... , \phi_d)$, where $d = 4p + 3q + 3$ is total number of parameters
to have a unique index:   


$$
\begin{aligned}
& \ \ \mathbf{(1)}  \quad \quad \quad  \quad  \quad \quad \quad ... \quad \quad  \quad \quad \quad \quad \quad \quad \quad  \quad \quad \quad  \quad  \quad \ ... \quad  \quad \quad \quad \quad \quad \quad  \quad \quad \quad \quad \quad \quad  \quad \quad \quad \ \ \  \mathbf{(d)} \\ \\
& \quad  \textbf{k}_1^* \quad  ...  \ \textbf{k}_q^*  \quad \quad \textbf{t}_{s1}^* \ ... \ \textbf{t}_{s(p+q)}^* \quad \textbf{a}_{s1}^* \ \ ... \ \ \textbf{a}_{s(p+q)}^* \quad \ \ \textbf{t}_{b1}^* \ \ ...  \ \textbf{t}_{bp}^* \quad \quad \ \ \ \textbf{a}_{b1}^* \ \ ... \ \textbf{a}_{bp}^*  \quad \quad \textbf{v}_s^* \quad \ \ \ \textbf{v}_b^* \quad \ \ \ \textbf{v}_e^* \\ \\
\Phi = 
&\begin{bmatrix} 
k_1^{(1)} \ ... \ k_q^{(1)} & t_{s1}^{(1)} \ ... \ t_{s(p+q)}^{(1)} & a_{s1}^{(1)} \ ... \ a_{s(p+q)}^{(1)} & t_{b1}^{(1)} \ ... \ t_{bp}^{(1)} & a_{b1}^{(1)} \ ... \ a_{bp}^{(1)}  & v_s^{(1)} & v_b^{(1)} & v_e^{(1)} \\ \\
... \  \    & \  \  & \ ... \  &  \  \  &  \ ... \ &  &  & ... \\ \\
k_1^{(N)} \ ... \ k_q^{(N)} & t_{s1}^{(N)} \ ... \ t_{s(p+q)}^{(N)} & a_{s1}^{(N)} \ ... \ a_{s(p+q)}^{(N)} & t_{b1}^{(N)} \ ... \ t_{b(p+q)}^{(N)} & a_{b1}^{(N)} \ ... \ a_{b(p+q)}^{(N)}  & v_s^{(N)} & v_b^{(N)} & v_e^{(N)} \\ 
\end{bmatrix} \\
\end{aligned} \\
$$
\linebreak
\begin{itemize}
\item Initialize the first row with user-given initial values:
$$
\begin{aligned}
\phi^{(1)} &= (\phi_1^{(1)}, \ ... , \ \phi_d^{(1)}) \\
& = (k_1, \ ... , \ k_q, \ \theta_0^{(1)}, \ ..., \ \theta_0^{(p+q)}, \ \alpha_0^{(1)}, \ ..., \ \alpha_0^{(p+q)}, \  \theta_0^{(1)}, \ ..., \ \theta_0^{(p)}, \ \alpha_0^{(p)}, \ ..., \ \alpha_0^{(p)}, \ v_{s0}, \ v_{b0}, \ v_{e0})
\end{aligned}
$$
\item At each iteration $i \in (2, ... , N)$, and to update $j$-th parameter ($j \in (1,... , d)$ and first ($j-1$) parameters are already updated):   
\begin{enumerate} 
\item Propose a new value for $\phi_j^{(i)}$ based on its last update \footnote{If it is first parameter, the last update is the last row (\(\phi^{(i-1)}\)}. $\phi_j^{(i-1)}$, called Metropolis Update (MU): 

$$\phi_j^* = \mathcal{N}(\phi_j^{(i-1)}, \sigma^2_p)$$
    
where $\sigma^2_p$ is adaptively adjusted \footnote{Every \(50\) iterations, acceptance rate (\(AR\)) is computed. If \(AR < 0.44\), proposal variance \(\sigma^2_p\) is decreased, and vice versa.It has been shown that for one-dimensional proposals used in Metropolis within Gibbs algorithm, the optimal acceptance rate is \(0.44\) (TODO: citation).} to ensure (faster) convergence.    

\item Form the parameter vector $\phi^*$ based on MU:
$$
\begin{aligned}
\phi^{(last)} &= (\phi_1^{(i)}, ... , \phi_{j-2}^{(i)}, \ \phi_{j-1}^{(i)}, \ \phi_{j}^{(i-1)}, \ ..., \ \phi_d^{(i-1)})    \\
\phi^*  \ \ \ \ \ &= (\phi_1^{(i)}, ... , \phi_{j-1}^{(i)}, \ \phi_{j}^*, \ \phi_{j+1}^{(i-1)}, \ ..., \ \phi_d^{(i-1)})    \\
\end{aligned}
$$ 
\item Draw a random sample $u$ from $U(0, 1)$ and take its log: $\ln(u)$  
\item Compute the difference between the log of joint posterior density given current   and last parameter vectors:
$$
h(\phi^*, \ \phi^{(last)}) = \ln(L(\text{y} | \phi^*)) + \ln(\mathcal{P}[\phi^*]) - \ln(L(\text{y} | \phi^{(last)})) + \ln(\mathcal{P}[\phi^{(last)}])
$$
\item If $h(\phi^*, \ \phi^{(last)}) > \ln(u)$, set:
$$
\phi_{j}^{(i)} = \phi_{j}^*
$$
otherwise, 
$$
\ \quad \phi_{j}^{(i)} = \phi_{j}^{(i-1)}
$$
\item The update vector now is:
$$
\phi^{(last)}  \ = (\phi_1^{(i)}, ... , \phi_{j-1}^{(i)}, \ \phi_{j}^{(i)}, \ \phi_{j+1}^{(i-1)}, \ ..., \ \phi_d^{(i-1)})
$$

\end{enumerate}

\item If all parameters are updated, go to next iteration of $i$
\item When iterations of $i$ is completed, return the matrix of $\Phi$ that 
contains joint posterior density distribution of all parameters. Marginal 
distribution of each parameter can be used for point prediction and uncertainty 
quantification (credible interval).
\end{itemize}

\rule{\linewidth}{2pt}


### A.2 Bayesian Analysis:

In the Bayesian framework, the joint probability distribution of all parameters
and hyperparameters of the calibration model given data 
($\mathcal{P}[\phi|\text{y}]$) can be derived:

$$
\begin{aligned}
L(\text{y} | \phi)                   &=       \ |C|^{-\frac{1}{2}} \ . \ e^{- \frac{1}{2} \text{y}. C^{-1}.\text{y}^T} \\ \\
\mathcal{P} [\phi | \text{y}]      \ &\propto \ L(\text{y} | \phi) \ .  \ \mathcal{P} [\phi]                         \\ \\
\end{aligned}
$$

Taking the log from both sides will decrease computational load and increase speed:


$$
\begin{aligned}
\ln(L(\text{y} | \phi))             &= -\frac{1}{2} \ln(|C|) - \frac{1}{2} \text{y}. C^{-1}.\text{y}^T \\ \\
\ln(\mathcal{P}[\phi | \text{y}])\  &\propto   \ln(L(\text{y} | \phi)) + \ln(\mathcal{P}[\phi) \\ \\
&= -\frac{1}{2} \ln(|C|) - \frac{1}{2} \text{y}. C^{-1}.\text{y}^T   &\text{(log liklihood given full data: X, y)} \\ \\
&+ \ \sum_{i=1}^q \mathcal{P}[\kappa_i]  &\text{(priors for calibration parameters)}\\ \\ 
&+ \ \sum_{i=1}^{p+q} \mathcal{P}[\theta_{si}] + \sum_{i=1}^{p+q} \mathcal{P}[\alpha_{si}] \ + \ \mathcal{P}[\sigma_s^2] \ &\text{(priors for} \ \eta(.) \ \text{hyperparameters)} \\ \\
&+ \ \sum_{i=1}^{p} \mathcal{P}[\theta_{bi}] + \sum_{i=1}^{p} \mathcal{P}[\alpha_{bi}] \ + \ \mathcal{P}[\sigma_b^2] &\text{(priors for} \ \delta_{\kappa}(.) \ \text{hyperparameters)}\\ \\
&+ \ \mathcal{P}[\sigma_{\epsilon}^2] &\text{(prior for measurement error variance)}\\ 
\end{aligned}
$$

Above equation is intractable and thus a simulation-based method must be used to
sample from posterior distribution. `FBC` implements a version Markov Chain 
Monte Carlo (MCMC) simulation. 


### Full Model

KOH model combines simulation and field data to form a joint model
using joint dataset:



$$
\begin{aligned}
\bf{y} \  &=
\begin{bmatrix}
\bf{y}_f  \\
\bf{y}_s \\ 
\end{bmatrix} = 
(y_1, \ ... \ , \ y_{n+m})^T
\ \ &\text{(joint vector of responses)}  \\ \\
\bf{X}   &=
\begin{bmatrix}
\bf{X_{\kappa}}  \\
\bf{X_s} \\
\end{bmatrix} =
\begin{bmatrix}
\bf{x_1} \ \bf{x_2}  \ ... \ \bf{x_{p+q}} \\
\end{bmatrix}
\ \ &\text{(joint input matrix)}  \\ \\
\bf{x_i} &= (x_1, x_2, ..., x_{n+m}) \ \ \ \ \forall i \in \{1, 2, ..., p+q\} \ 
\end{aligned}
$$


Since $\bf{X_f}$ is a sub-matrix of $\bf{X}$, we can represent the functional
relationship between input and response for full model with $\zeta(.)$, which
is considered to be realization of a random function and derived from $\eta(.)$
and $\delta_{\kappa}(.)$:

$$
\bf{y} \  = \zeta(\bf{X}) 
$$



Because both $\eta(.)$ and $\delta_{\kappa}(.)$ are GPs, $\zeta(.)$ can also be
considered a zero mean GP:

$$
\zeta(.) \sim GP \ (0, \ C(., .))
$$

Where covariance matrix $C$ is dependent to full input matrix $\bf{X}$ and 
hyperparameters of $\eta(.)$ and $\delta_{\kappa}(.)$, which in turn are 
dependent to model hyperparameters.


Using matrix notation, input/output relationship of both experiments is 
presented below in matrix notation. This relations can be used to derive a 
relationship for joint data:

$$
\text{y} \  = \zeta(X) = \eta(X) + 
\begin{bmatrix}
\delta_{\kappa}(X_f) +\mathcal{E} & 0 \\
0 & 0  \\
\end{bmatrix} \\ \\
$$

Since $X_f$ is a subset of matrix $X$, the joint response $\text{y}$ can be 
modeled as a random function $\zeta(X)$. In the ball example, the full input 
matrix $X$ is a $(163 \times 2)$ matrix by stacking $X_{\kappa}$ on $X_s$ Note 
that since $\kappa$ is unknown, the initial value `c0` is used internally to 
build $X_{\kappa}$.






Where, $C_{\eta}$ and $C_{\delta}$ are covariance matrices of full input matrix 
$X$ and original field input matrix $X_f$. And $C$ is characterized as:


$$
\begin{aligned}
C(X, X) &=  C_{\eta}(X, X) + 
\begin{bmatrix} 
C_{\delta}(X_f, X_f) + \sigma^2_{\epsilon}.I_n   &   0  \\
0                               &   0 \\
\end{bmatrix} \\ \\ 
&= 
\begin{bmatrix} 
C_{\eta}(X_f, X_f) + C_{\delta} + \sigma^2_{\epsilon}.I_n & C_{\eta}(X_s, X_f)  \\ 
C_{\eta}(X_f, X_s)                                & C_{\eta}(X_s, X_s)  \\
\end{bmatrix}
\end{aligned}
$$

Note that $C_s$ covariance matrix of the full data $X$ is further divided to
components $C_{\eta}(X_f, X_f)$, $C_{\eta}(X_s, X_f)$, $C_{\eta}(X_f, X_s)$, and
$C_{\eta}(X_s, X_s)$ to optimize the computation. 



## References

> "He who gives up [code] safety for [code] speed deserves neither."
([via](https://twitter.com/hadleywickham/status/504368538874703872))










### KOH Calibration Model

Kennedy and O'Hagan (KOH) proposed a Bayesian calibration model to integrate the 
information from field observations, $y_f$, (and its corresponding matrix of 
experimental input, $X_f$) and computer simulation, $y_s$ (and its corresponding 
matrix of experimental/calibration input, $X_s$). Often, in computer simulation
there are extra parameters that are either are unkwon in the field setting or
are tuning parameters specific to simulation without physical interpretation. 
These inputs are calibration parameters and tuning them for the computer model is
one of the main objective of calibration. 

Moreover, KOH assumes both field and simulation results are realization of real 
process, $Y$, while admitting to a systemic bias in simulation models, and 
measurement error in the form of independent zero-mean Gaussian error terms with
variance $\sigma^2_{\epsilon}$.:  


$$
\begin{eqnarray}
Y|_{X_f} : y(x) &=& y_f(x_f) +  \epsilon \ \ &, \text{where} \ x_f: 1 \times p \ \text{vector of experimental inputs} \\
Y|_{X_s} : y(x) &=& y_s(x_s) + y_b(x_b) + \epsilon \ \ &, \ \text{where} \ x_s: 1 \times p+q \ \text{vector of experimental + calibration inputs}
\end{eqnarray}
$$

In this formulation, $x_f$ is in fact $1 \times p+q$ vector, similar to $x_s$, 
because it is realization of the same real process. However, calibration 
parameters are set (fixed or variable) at their best value but their value is 
not known. Since all field observation have the same calibration parameters,
they are removed from notation. The current text uses $(1 \times p+q)$ vector 
$x_s$ for simulation input, $(1 \times p+q)$ vector $x_f$ for field data 
augmented with calibration parameters (at their current updated value and same 
for all field observations), and $(1 \times p)$ vector $x_b$ for field data 
without augmenting calibration parameters. The subscript $b$ is chosen to 
indicate that it is only input to bias term. Stacking the two data, we can write
the KOH calibration model in a single expression:

$$
Y|_{X} : y(x) = y_s(x) + y_b(x) + \epsilon \ \ , \ \text{where} \ x: 1 \times p+q \ \text{vector of experimental + calibration inputs}
$$

this formulation, emphasizes that model has systemic bias, even under best 
calibration parameters. However, KOH postulates that reasonable correction can
be made through $y_b(.)$ 

Above specification leads to three unknowns: calibration parameters (denoted by
$\kappa$), variance of measurement error ($\sigma^2_{\epsilon}$) and functional
form of bias correction term (denoted by $y_b(.)$ noting that the actual bias is
$-y_b(.)$). Moreover, KOH assumes a Gaussian Process (GP) for both simulation
term (often the simulation is also expensive leading to limited runs. Fitting a 
GP as a surrogate to couble bias-correction GP) and bias-correction term, which 
introduces new hyper-parameters to the model. 

Specifically, assuming a generalized correlation structure such as power 
exponential correlation family, the scale (denoted by $\theta_s$ and $\theta_b$),
smoothness (denoted by $\alpha_s$ and $\alpha_b$), and their marginal variance
(denoted by $\sigma_s^2$ and $\sigma_b^2$) are added to the model. 

$$
\Omega = (Y_s(.), \Delta_b(.), \kappa, \sigma_{\epsilon}^2 ) = (\kappa, \theta_s, \alpha_s, \sigma_s^2, \theta_b, \alpha_b, \sigma^2_b, \sigma_{\epsilon}^2)
$$

After loading of the data (either toy data or user data), the combined 
observation vector is scaled by mean and standard deviation of simulation runs 
such that $y_s$ terms have mean zero and standard deviation of 1 after scaling.
Furthermore, input matrix is scaled according to mean and standard deviation of
columns of the input matrix (consisting of experimental and calibration values)
such that $X_s$ columns span $[0, 1]$. These scaling removes the need for 
parametrization of the mean of the GPs as they will be considered zero. 
Following the results from Chen et al. (2016), the fitting a linear regression
model does not necessarily improve the performance as the GP models are flexible.
Therefore current package does not consider fitting regression model on the mean
GPs.

### Bayesian Inference

The framework of KOH model is Bayesian. Prior information and restrictions about
model parameters and hyperparameters can be specified through informative or
non-informative priors. 

For calibration parameters, often non-informative uniform prior is used over 
wide but finite domain, unless expert opinion is available in the form of prior
distributions. An alternative choice can be a regularizing prior that prevent 
from over-concentration of posterior density on boundaries of domain.

... other pror specification

The likelihood for posterior density is in following form along with its log 
form (where both sides are log transformed for computational efficiency):


$$
\begin{align}
&L(\Omega \mid Y) = 
\end{align}
$$
where augmented covariance matrix is given by:

$$
\Sigma \mid \Omega = 
$$

This expression is interactable and must be solved through optimization methods
or it can be used to sample from its joint posterior.


### Monte Carlo Markov Chain Sampling

### Metropolis-Within-Gibbs Algorithm Imlementation

### Adaptive Proposal

### Parameter Estimation

### Prediction

### Toy Example

##### Data

##### Model

##### MCMC Runs

##### Parameter Estimation

##### Prediction

##### Diagnostics

### Future Development

### Consclusion






(denoted by $\alpha$) and marginal variance













